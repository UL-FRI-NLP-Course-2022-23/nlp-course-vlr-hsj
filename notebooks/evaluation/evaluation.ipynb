{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luka\\miniconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_23996\\2349417202.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "bert = load_metric(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(decoded_preds, decoded_labels, prediction_lens):\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5-SL-SMALL\n",
      "{'rouge1': 0.3381, 'rouge2': 0.0651, 'rougeL': 0.2428, 'rougeLsum': 0.3114, 'gen_len': 3.4974}\n",
      "{'Precision': 0.6246253614609059, 'Recall': 0.440937814724751, 'F1': 0.5145365306108426}\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"cjvt/t5-sl-small\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "\n",
    "data = pd.read_csv(data_path, sep=\";\")\n",
    "# display(data)\n",
    "\n",
    "decoded_labels = data[\"reply\"].to_list()\n",
    "decoded_preds = data[\"generated\"].to_list()\n",
    "prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "quantitative_data = pd.DataFrame({\"decoded_preds\": data[\"generated\"], \"decoded_labels\": data[\"reply\"], \"prediction_lens\": data[\"token_prediction_len\"]})\n",
    "# display(quantitative_data)\n",
    "\n",
    "print(model_name.upper())\n",
    "\n",
    "# ROUGE\n",
    "result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "print(result_rouge)\n",
    "\n",
    "# BERTSCORE\n",
    "result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "print(result_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 small finetuned pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5-SL-SMALL-FINETUNED-ASSISTANT\n",
      "{'rouge1': 17.2217, 'rouge2': 4.0804, 'rougeL': 11.4804, 'rougeLsum': 15.7287, 'gen_len': 73.3327}\n",
      "{'Precision': 0.669749138269669, 'Recall': 0.6337999112606049, 'F1': 0.6494640069435804}\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"ls1906/t5-sl-small-finetuned-assistant\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "\n",
    "data = pd.read_csv(data_path, sep=\";\")\n",
    "# display(data)\n",
    "\n",
    "decoded_labels = data[\"reply\"].to_list()\n",
    "decoded_preds = data[\"generated\"].to_list()\n",
    "prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "quantitative_data = pd.DataFrame({\"decoded_preds\": data[\"generated\"], \"decoded_labels\": data[\"reply\"], \"prediction_lens\": data[\"token_prediction_len\"]})\n",
    "# display(quantitative_data)\n",
    "\n",
    "print(model_name.upper())\n",
    "\n",
    "# ROUGE\n",
    "result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "print(result_rouge)\n",
    "\n",
    "# BERTSCORE\n",
    "result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "print(result_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"TODO\"\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "\n",
    "# data = pd.read_csv(data_path, sep=\";\")\n",
    "# # display(data)\n",
    "\n",
    "# decoded_labels = data[\"reply\"].to_list()\n",
    "# decoded_preds = data[\"generated\"].to_list()\n",
    "# prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "# quantitative_data = pd.DataFrame({\"decoded_preds\": data[\"generated\"], \"decoded_labels\": data[\"reply\"], \"prediction_lens\": data[\"token_prediction_len\"]})\n",
    "# # display(quantitative_data)\n",
    "\n",
    "# print(model_name.upper())\n",
    "\n",
    "# # ROUGE\n",
    "# result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "# print(result_rouge)\n",
    "\n",
    "# # BERTSCORE\n",
    "# result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "# result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "# print(result_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT finetuned pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-SL-OASST1-PAIRS\n",
      "{'rouge1': 16.8979, 'rouge2': 2.625, 'rougeL': 9.8226, 'rougeLsum': 15.6079, 'gen_len': 162.5594}\n",
      "{'Precision': 0.6103136431987469, 'Recall': 0.6375802568716881, 'F1': 0.6223735224894988}\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"vh-student/gpt-sl-oasst1-pairs\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "\n",
    "data = pd.read_csv(data_path, sep=\";\")\n",
    "# display(data)\n",
    "\n",
    "decoded_labels = data[\"reply\"].to_list()\n",
    "decoded_preds = data[\"generated\"].to_list()\n",
    "prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "quantitative_data = pd.DataFrame({\"decoded_preds\": data[\"generated\"], \"decoded_labels\": data[\"reply\"], \"prediction_lens\": data[\"token_prediction_len\"]})\n",
    "# display(quantitative_data)\n",
    "\n",
    "print(model_name.upper())\n",
    "\n",
    "# ROUGE\n",
    "result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "print(result_rouge)\n",
    "\n",
    "# BERTSCORE\n",
    "result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "print(result_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT finetuned context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"TODO\"\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "\n",
    "# data = pd.read_csv(data_path, sep=\";\")\n",
    "# # display(data)\n",
    "\n",
    "# decoded_labels = data[\"reply\"].to_list()\n",
    "# decoded_preds = data[\"generated\"].to_list()\n",
    "# prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "# quantitative_data = pd.DataFrame({\"decoded_preds\": data[\"generated\"], \"decoded_labels\": data[\"reply\"], \"prediction_lens\": data[\"token_prediction_len\"]})\n",
    "# # display(quantitative_data)\n",
    "\n",
    "# print(model_name.upper())\n",
    "\n",
    "# # ROUGE\n",
    "# result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "# print(result_rouge)\n",
    "\n",
    "# # BERTSCORE\n",
    "# result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "# result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "# print(result_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"TODO\"\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "\n",
    "# data = pd.read_csv(data_path, sep=\";\")\n",
    "# # display(data)\n",
    "\n",
    "# decoded_labels = data[\"reply\"].to_list()\n",
    "# decoded_preds = data[\"generated\"].to_list()\n",
    "# prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "# quantitative_data = pd.DataFrame({\"decoded_preds\": data[\"generated\"], \"decoded_labels\": data[\"reply\"], \"prediction_lens\": data[\"token_prediction_len\"]})\n",
    "# # display(quantitative_data)\n",
    "\n",
    "# print(model_name.upper())\n",
    "\n",
    "# # ROUGE\n",
    "# result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "# print(result_rouge)\n",
    "\n",
    "# # BERTSCORE\n",
    "# result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "# result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "# print(result_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 large finetuned pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"TODO\"\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "\n",
    "# data = pd.read_csv(data_path, sep=\";\")\n",
    "# # display(data)\n",
    "\n",
    "# decoded_labels = data[\"reply\"].to_list()\n",
    "# decoded_preds = data[\"generated\"].to_list()\n",
    "# prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "# quantitative_data = pd.DataFrame({\"decoded_preds\": data[\"generated\"], \"decoded_labels\": data[\"reply\"], \"prediction_lens\": data[\"token_prediction_len\"]})\n",
    "# # display(quantitative_data)\n",
    "\n",
    "# print(model_name.upper())\n",
    "\n",
    "# # ROUGE\n",
    "# result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "# print(result_rouge)\n",
    "\n",
    "# # BERTSCORE\n",
    "# result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "# result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "# print(result_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 large finetuned context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"TODO\"\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "\n",
    "# data = pd.read_csv(data_path, sep=\";\")\n",
    "# # display(data)\n",
    "\n",
    "# decoded_labels = data[\"reply\"].to_list()\n",
    "# decoded_preds = data[\"generated\"].to_list()\n",
    "# prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "# quantitative_data = pd.DataFrame({\"decoded_preds\": data[\"generated\"], \"decoded_labels\": data[\"reply\"], \"prediction_lens\": data[\"token_prediction_len\"]})\n",
    "# # display(quantitative_data)\n",
    "\n",
    "# print(model_name.upper())\n",
    "\n",
    "# # ROUGE\n",
    "# result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "# print(result_rouge)\n",
    "\n",
    "# # BERTSCORE\n",
    "# result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "# result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "# print(result_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
