{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "bert = load_metric(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(decoded_preds, decoded_labels, prediction_lens):\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints = [\n",
    "    \"cjvt/t5-sl-small\",\n",
    "    \"ls1906/t5-sl-small-finetuned-assistant\",\n",
    "    \"vh-student/gpt-sl-oasst1-pairs\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5-SL-SMALL\n",
      "{'rouge1': 0.3381, 'rouge2': 0.0651, 'rougeL': 0.2428, 'rougeLsum': 0.3114, 'gen_len': 3.4974}\n",
      "{'Precision': 0.6246253614609059, 'Recall': 0.440937814724751, 'F1': 0.5145365306108426}\n",
      "T5-SL-SMALL-FINETUNED-ASSISTANT\n",
      "{'rouge1': 17.2265, 'rouge2': 4.0796, 'rougeL': 11.4772, 'rougeLsum': 15.7305, 'gen_len': 73.2593}\n",
      "{'Precision': 0.6693199212489984, 'Recall': 0.6336666443225665, 'F1': 0.6491875979411297}\n",
      "GPT-SL-OASST1-PAIRS\n",
      "{'rouge1': 16.8979, 'rouge2': 2.625, 'rougeL': 9.8226, 'rougeLsum': 15.6079, 'gen_len': 162.5594}\n",
      "{'Precision': 0.6103136431987469, 'Recall': 0.6375802568716881, 'F1': 0.6223735224894988}\n"
     ]
    }
   ],
   "source": [
    "for model_checkpoint in model_checkpoints:\n",
    "    # read local data depending on the model that it was generated with\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "    data = pd.read_csv(data_path, sep=\";\")\n",
    "\n",
    "    decoded_labels = data[\"reply\"].to_list()\n",
    "    decoded_preds = data[\"generated\"].to_list()\n",
    "    prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "\n",
    "    print(model_name.upper())\n",
    "\n",
    "    # ROUGE\n",
    "    result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "    print(result_rouge)\n",
    "\n",
    "    # BERTSCORE\n",
    "    result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "    result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "    print(result_bert)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
