{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luka\\miniconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import *\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\AppData\\Local\\Temp\\ipykernel_3120\\2349417202.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "bert = load_metric(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(decoded_preds, decoded_labels, prediction_lens):\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints = [\n",
    "    \"cjvt/t5-sl-small\",\n",
    "    \"ls1906/t5-sl-small-finetuned-assistant\",\n",
    "    # \"cjvt/t5-sl-large\",\n",
    "    # \"vh-student/t5-sl-large-oasst-pairs\",\n",
    "    # \"vh-student/t5-sl-large-oasst-context\",\n",
    "    \"cjvt/gpt-sl-base\",\n",
    "    \"vh-student/gpt-sl-oasst1-pairs\",\n",
    "    \"vh-student/gpt-sl-oasst1-context\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5-SL-SMALL\n",
      "{'rouge1': 0.3373, 'rouge2': 0.0651, 'rougeL': 0.243, 'rougeLsum': 0.3124, 'gen_len': 3.4974}\n",
      "{'Precision': 0.6246253633865944, 'Recall': 0.44093781444965263, 'F1': 0.5145365309715271}\n",
      "\n",
      "T5-SL-SMALL-FINETUNED-ASSISTANT\n",
      "{'rouge1': 17.2219, 'rouge2': 4.0704, 'rougeL': 11.476, 'rougeLsum': 15.7317, 'gen_len': 73.2593}\n",
      "{'Precision': 0.6693199218603281, 'Recall': 0.6336666446221181, 'F1': 0.6491875983446073}\n",
      "\n",
      "GPT-SL-BASE\n",
      "{'rouge1': 15.0181, 'rouge2': 2.4436, 'rougeL': 9.1311, 'rougeLsum': 13.5434, 'gen_len': 162.5594}\n",
      "{'Precision': 0.5951044182532873, 'Recall': 0.6274400653044383, 'F1': 0.6096532132686713}\n",
      "\n",
      "GPT-SL-OASST1-PAIRS\n",
      "{'rouge1': 16.8995, 'rouge2': 2.627, 'rougeL': 9.8213, 'rougeLsum': 15.6109, 'gen_len': 162.5594}\n",
      "{'Precision': 0.6103136447759775, 'Recall': 0.637580257342412, 'F1': 0.6223735225873116}\n",
      "\n",
      "GPT-SL-OASST1-CONTEXT\n",
      "{'rouge1': 14.7758, 'rouge2': 2.3965, 'rougeL': 8.8418, 'rougeLsum': 13.6492, 'gen_len': 202.3118}\n",
      "{'Precision': 0.6056029972235631, 'Recall': 0.6519947136428369, 'F1': 0.6259877759173674}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_checkpoint in model_checkpoints:\n",
    "    # read local data depending on the model that it was generated with\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    data_path = f\"../../data/results/prompt_reply_pairs_1_generated_{split}_{model_name}.csv\"\n",
    "    data = pd.read_csv(data_path, sep=\";\")\n",
    "\n",
    "    decoded_labels = data[\"reply\"].to_list()\n",
    "    decoded_preds = data[\"generated\"].to_list()\n",
    "    prediction_lens = data[\"token_prediction_len\"].to_list()\n",
    "\n",
    "    print(model_name.upper())\n",
    "\n",
    "    # ROUGE\n",
    "    result_rouge = compute_rouge(decoded_preds, decoded_labels, prediction_lens)\n",
    "    print(result_rouge)\n",
    "\n",
    "    # BERTSCORE\n",
    "    result_bert = bert.compute(predictions=decoded_preds, references=decoded_labels, lang=\"sl\")\n",
    "    result_bert = {\"Precision\": np.array(result_bert[\"precision\"]).mean(), \"Recall\": np.array(result_bert[\"recall\"]).mean(), \"F1\": np.array(result_bert[\"f1\"]).mean()}\n",
    "    print(result_bert)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RRHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
