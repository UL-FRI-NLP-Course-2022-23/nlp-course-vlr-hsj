{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "data_path_pairs = '../../data/results/prompt_reply_pairs_5_generated_test_t5-sl-small.csv'\n",
    "step = 5\n",
    "\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path_pairs, sep=\";\")\n",
    "eval_data = Dataset.from_pandas(data[[\"prompt\", \"generated\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): CamembertClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vh-student/sloberta-si-rrhf\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vh-student/sloberta-si-rrhf\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(examples):\n",
    "    prefix_in = \"UPORABNIK: \"\n",
    "    examples[\"prompt\"] = [prefix_in + prompt for prompt in examples[\"prompt\"]]\n",
    "    prefix_out = \"ASISTENT: \"\n",
    "    examples[\"generated\"] = [prefix_out + reply for reply in examples[\"generated\"]]\n",
    "    \n",
    "    examples[\"PROMPT\"] = [prompt + \" \" + reply for prompt, reply in zip(examples[\"prompt\"], examples[\"generated\"])]\n",
    "    model_inputs = tokenizer(examples['PROMPT'], pad_to_max_length=True, max_length=512, truncation=True, return_tensors='pt')\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/24375 [00:00<?, ? examples/s]/home/valter/conda/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "eval_data = eval_data.map(convert_to_features, batched=True, load_from_cache_file=False)\n",
    "eval_data.set_format(type=\"torch\", columns=[\"prompt\", \"generated\", \"PROMPT\", \"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4875/4875 [05:28<00:00, 14.85it/s]\n"
     ]
    }
   ],
   "source": [
    "indices = np.concatenate([np.arange(0, eval_data.num_rows, step), [eval_data.num_rows]])\n",
    "\n",
    "outputs = []\n",
    "model = model.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(indices[:-1]))):\n",
    "        scores = []\n",
    "        for j in range(step):\n",
    "            j = int(j)\n",
    "            with torch.no_grad():\n",
    "                outputs_proba = model(input_ids = eval_data[int(indices[i]) + j][\"input_ids\"].unsqueeze(dim = 1).to(DEVICE), \n",
    "                                    attention_mask = eval_data[int(indices[i]) + j][\"attention_mask\"].unsqueeze(dim = 1).to(DEVICE))\n",
    "            probas = torch.nn.functional.softmax(outputs_proba.logits, dim=1).cpu().detach().numpy()[0]\n",
    "            p = probas[0] ** 1 + probas[1] ** 2 + probas[2] ** 3 + probas[3] ** 4 + probas[4] ** 5\n",
    "            scores.append(p)\n",
    "        P = np.argmax(np.array(scores))\n",
    "        outputs.append([eval_data[i + int(P)][\"prompt\"], eval_data[i + int(P)][\"generated\"].replace(\"ASISTENT: \", \"\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame(outputs, columns=[\"prompt\", \"generated\"])\n",
    "outputs.to_csv(f\"{data_path_pairs.split('.')[0]}_best.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
