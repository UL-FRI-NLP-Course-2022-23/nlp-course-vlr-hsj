{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\"\n",
    "data_path_pairs = '../../data/results/prompt_reply_pairs_5_generated_test_t5-sl-small.csv'\n",
    "step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path_pairs, sep=\";\")\n",
    "eval_data = Dataset.from_pandas(data[[\"prompt\", \"generated\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vh-student/sloberta-si-rrhf\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vh-student/sloberta-si-rrhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(examples):\n",
    "    prefix_in = \"UPORABNIK: \"\n",
    "    examples[\"prompt\"] = [prefix_in + prompt for prompt in examples[\"prompt\"]]\n",
    "    prefix_out = \"ASISTENT: \"\n",
    "    examples[\"generated\"] = [prefix_out + reply for reply in examples[\"generated\"]]\n",
    "    \n",
    "    examples[\"PROMPT\"] = [prompt + \" \" + reply for prompt, reply in zip(examples[\"prompt\"], examples[\"generated\"])]\n",
    "    model_inputs = tokenizer(examples['PROMPT'], pad_to_max_length=True, max_length=512, truncation=True, return_tensors='pt')\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = eval_data.map(convert_to_features, batched=True, load_from_cache_file=False)\n",
    "eval_data.set_format(type=\"torch\", columns=[\"prompt\", \"generated\", \"PROMPT\", \"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.concatenate([np.arange(0, eval_data.num_rows, step), [eval_data.num_rows]])\n",
    "\n",
    "outputs = []\n",
    "for i in tqdm(range(len(indices[:-1]))):\n",
    "    scores = []\n",
    "    for j in range(step):\n",
    "        with torch.no_grad():\n",
    "            outputs_proba = model(input_ids = eval_data[int(indices[i]) + j][\"input_ids\"].unsqueeze(dim = 1), \n",
    "                                  attention_mask = eval_data[int(indices[i]) + j][\"attention_mask\"].unsqueeze(dim = 1))\n",
    "        probas = torch.nn.functional.softmax(outputs_proba.logits, dim=1).detach().numpy()[0]\n",
    "        p = probas[0] ** 1 + probas[1] ** 2 + probas[2] ** 3 + probas[3] ** 4 + probas[4] ** 5\n",
    "        scores.append(p)\n",
    "    P = np.argmax(np.array(scores))\n",
    "    outputs.append([eval_data[i + int(P)][\"prompt\"], eval_data[i + int(P)][\"generated\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame(outputs, columns=[\"prompt\", \"generated\"])\n",
    "outputs.to_csv(f\"{data_path_pairs.split('.')[0]}_best.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_ds_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
