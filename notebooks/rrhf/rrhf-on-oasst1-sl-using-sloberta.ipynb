{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-26T13:37:28.429709Z","iopub.status.busy":"2023-05-26T13:37:28.429348Z","iopub.status.idle":"2023-05-26T13:37:28.440829Z","shell.execute_reply":"2023-05-26T13:37:28.439771Z","shell.execute_reply.started":"2023-05-26T13:37:28.429680Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pytorch_lightning'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloggers\u001b[39;00m \u001b[39mimport\u001b[39;00m WandbLogger\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader, Dataset\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"]}],"source":["import torch\n","import pandas as pd\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import AdamW\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n","\n","wandb_logger = WandbLogger(project=\"RRHF\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T12:17:42.419607Z","iopub.status.busy":"2023-05-26T12:17:42.419208Z","iopub.status.idle":"2023-05-26T12:17:49.350735Z","shell.execute_reply":"2023-05-26T12:17:49.349548Z","shell.execute_reply.started":"2023-05-26T12:17:42.419578Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f068e3a4e8442069990b82d3e5f39ca","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/933 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01283f6dbded42228707f3a16b2e123c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c9ffe29896b42aa8dee27aa59939484","version_major":2,"version_minor":0},"text/plain":["Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/800k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6f4aeabf8de445db78a10f6e463a952","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.34M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac55a920043744e496265d3e7b54c7e2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/353 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa7c25c0e12f4572b02edf183edfc41c","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at cjvt/sloberta-si-nli and are newly initialized because the shapes did not match:\n","- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n","- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Set the number of categories\n","num_categories = 5\n","\n","# Load the base model configuration\n","config = AutoConfig.from_pretrained(\"cjvt/sloberta-si-nli\")\n","config.num_labels = num_categories\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"cjvt/sloberta-si-nli\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"cjvt/sloberta-si-nli\", config=config, ignore_mismatched_sizes=True)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T13:37:48.658405Z","iopub.status.busy":"2023-05-26T13:37:48.657306Z","iopub.status.idle":"2023-05-26T13:37:50.492892Z","shell.execute_reply":"2023-05-26T13:37:50.491638Z","shell.execute_reply.started":"2023-05-26T13:37:48.658363Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/oasst1-sl/context_reply_pairs.csv', sep=';')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T13:37:52.969942Z","iopub.status.busy":"2023-05-26T13:37:52.969538Z","iopub.status.idle":"2023-05-26T13:37:52.994956Z","shell.execute_reply":"2023-05-26T13:37:52.991731Z","shell.execute_reply.started":"2023-05-26T13:37:52.969911Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>reply</th>\n","      <th>rank</th>\n","      <th>split</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>UPORABNIK: V mojo hišo bodo prispeli 3 prijate...</td>\n","      <td>ASISTENT: Kaj pa igranje lažnivca? Za to igro ...</td>\n","      <td>0</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>UPORABNIK: V mojo hišo bodo prispeli 3 prijate...</td>\n","      <td>ASISTENT: Uno: Je klasična igra za otroke in o...</td>\n","      <td>1</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>UPORABNIK: V mojo hišo bodo prispeli 3 prijate...</td>\n","      <td>ASISTENT: Igrate lahko:\\r\\n - 8 noro\\r\\n - cel...</td>\n","      <td>2</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>UPORABNIK: Napišite hvalnico šolskemu taboru v...</td>\n","      <td>ASISTENT: Spomnim se vas prijatelji\\r\\nIn spet...</td>\n","      <td>0</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>UPORABNIK: Ali kaj bolj veselega?</td>\n","      <td>ASISTENT: ♪ Spakirajte in se pridružite zabavi...</td>\n","      <td>0</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              prompt  \\\n","0  UPORABNIK: V mojo hišo bodo prispeli 3 prijate...   \n","1  UPORABNIK: V mojo hišo bodo prispeli 3 prijate...   \n","2  UPORABNIK: V mojo hišo bodo prispeli 3 prijate...   \n","3  UPORABNIK: Napišite hvalnico šolskemu taboru v...   \n","4                  UPORABNIK: Ali kaj bolj veselega?   \n","\n","                                               reply  rank  split  \n","0  ASISTENT: Kaj pa igranje lažnivca? Za to igro ...     0  train  \n","1  ASISTENT: Uno: Je klasična igra za otroke in o...     1  train  \n","2  ASISTENT: Igrate lahko:\\r\\n - 8 noro\\r\\n - cel...     2  train  \n","3  ASISTENT: Spomnim se vas prijatelji\\r\\nIn spet...     0  train  \n","4  ASISTENT: ♪ Spakirajte in se pridružite zabavi...     0  train  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["data.head()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T13:37:59.536026Z","iopub.status.busy":"2023-05-26T13:37:59.535360Z","iopub.status.idle":"2023-05-26T13:37:59.703260Z","shell.execute_reply":"2023-05-26T13:37:59.701905Z","shell.execute_reply.started":"2023-05-26T13:37:59.535989Z"},"trusted":true},"outputs":[],"source":["data.loc[data['rank'] > 4, 'rank'] = 4\n","data['combined'] = \"UPORABNIK: \" + data['prompt'] + \" ASISTENT: \" + data['reply']"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T13:38:01.452291Z","iopub.status.busy":"2023-05-26T13:38:01.451833Z","iopub.status.idle":"2023-05-26T13:38:01.542289Z","shell.execute_reply":"2023-05-26T13:38:01.540931Z","shell.execute_reply.started":"2023-05-26T13:38:01.452241Z"},"trusted":true},"outputs":[],"source":["train_data = data[data.split == 'train']\n","val_data   = data[data.split == 'val']\n","\n","train_texts = train_data['combined'].tolist()\n","train_labels = train_data['rank'].tolist()\n","\n","val_texts = val_data['combined'].tolist()\n","val_labels = val_data['rank'].tolist()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T13:38:03.722999Z","iopub.status.busy":"2023-05-26T13:38:03.722632Z","iopub.status.idle":"2023-05-26T13:38:43.096587Z","shell.execute_reply":"2023-05-26T13:38:43.095303Z","shell.execute_reply.started":"2023-05-26T13:38:03.722969Z"},"trusted":true},"outputs":[],"source":["# Tokenize the text data\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","\n","# Convert labels to numerical format\n","train_labels = torch.tensor(train_labels)\n","val_labels = torch.tensor(val_labels)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T13:38:43.098947Z","iopub.status.busy":"2023-05-26T13:38:43.098603Z","iopub.status.idle":"2023-05-26T13:38:45.477079Z","shell.execute_reply":"2023-05-26T13:38:45.475424Z","shell.execute_reply.started":"2023-05-26T13:38:43.098911Z"},"trusted":true},"outputs":[],"source":["# Create a custom dataset class\n","class ReplyClassificationDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    \n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = self.labels[idx]\n","        return item\n","    \n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = ReplyClassificationDataset(train_encodings, train_labels)\n","val_dataset = ReplyClassificationDataset(val_encodings, val_labels)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T12:18:28.344466Z","iopub.status.busy":"2023-05-26T12:18:28.343493Z","iopub.status.idle":"2023-05-26T12:18:28.358044Z","shell.execute_reply":"2023-05-26T12:18:28.356754Z","shell.execute_reply.started":"2023-05-26T12:18:28.344431Z"},"trusted":true},"outputs":[],"source":["class ReplyClassificationModel(pl.LightningModule):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","\n","    def forward(self, **inputs):\n","        return self.model(**inputs)\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids = batch['input_ids']\n","        attention_mask = batch['attention_mask']\n","        labels = batch['labels']\n","\n","        outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = torch.nn.functional.cross_entropy(outputs.logits, labels)\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids = batch['input_ids']\n","        attention_mask = batch['attention_mask']\n","        labels = batch['labels']\n","\n","        outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = torch.nn.functional.cross_entropy(outputs.logits, labels)\n","\n","        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","\n","\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=1e-5)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T12:18:29.551027Z","iopub.status.busy":"2023-05-26T12:18:29.550346Z","iopub.status.idle":"2023-05-26T12:18:30.146856Z","shell.execute_reply":"2023-05-26T12:18:30.145740Z","shell.execute_reply.started":"2023-05-26T12:18:29.550992Z"},"trusted":true},"outputs":[],"source":["# Initialize the PyTorch Lightning model\n","pl_model = ReplyClassificationModel(model)\n","\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    dirpath=\"/kaggle/working\",\n","    filename='best-model',\n","    save_top_k=2,\n","    verbose=True,\n","    monitor=\"val_loss\",\n","    mode=\"min\"\n",")\n","\n","# Set up the PyTorch Lightning trainer\n","trainer = pl.Trainer(accelerator=\"auto\", \n","                     devices=2, \n","                     max_epochs=2,\n","                     callbacks = checkpoint,\n","                     logger=wandb_logger)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T12:18:34.717904Z","iopub.status.busy":"2023-05-26T12:18:34.717508Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /kaggle/working exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","  warning_cache.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c24b317665394522bfe222045bcd47dd","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","  warning_cache.warn(\n"]}],"source":["# Train the model\n","trainer.fit(pl_model, DataLoader(train_dataset, batch_size=8, shuffle=True), DataLoader(val_dataset, batch_size=16))"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T13:38:45.479380Z","iopub.status.busy":"2023-05-26T13:38:45.478708Z","iopub.status.idle":"2023-05-26T13:38:45.500370Z","shell.execute_reply":"2023-05-26T13:38:45.497751Z","shell.execute_reply.started":"2023-05-26T13:38:45.479335Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'UPORABNIK: UPORABNIK: Mi lahko poveste, katera odločitev vrhovnega sodišča je dala ljudem v Združenih državah individualno pravico do nošenja strelnega orožja? Ali lahko povzamete primer in tudi odločitev? ASISTENT: ASISTENT: Odločitev vrhovnega sodišča, ki je ljudem v Združenih državah dala individualno pravico do nošenja strelnega orožja, je bila District of Columbia proti Hellerju (2008). Primer je vložil Dick Heller, prebivalec okrožja Columbia, ki je izpodbijal prepoved posedovanja pištol v okrožju. Vrhovno sodišče je razsodilo, da drugi amandma ščiti posameznikovo pravico do posedovanja strelnega orožja, ki ni povezano s službo v milici, in da okrožna prepoved posedovanja pištole krši to pravico.'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["k = 422\n","train_data.iloc[k].combined"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = 'sloberta-si-rrhf-context'\n","pl_model.model.push_to_hub(model_name)\n","tokenizer.push_to_hub(model_name)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-26T13:39:01.977197Z","iopub.status.busy":"2023-05-26T13:39:01.976818Z","iopub.status.idle":"2023-05-26T13:39:04.146957Z","shell.execute_reply":"2023-05-26T13:39:04.145752Z","shell.execute_reply.started":"2023-05-26T13:39:01.977166Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted label: 1\n","Probabilities: tensor([[0.2470, 0.4846, 0.2554, 0.0101, 0.0028]])\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"vh-student/sloberta-si-rrhf\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"vh-student/sloberta-si-rrhf\")\n","\n","# Define your custom reply\n","custom_reply = '''UPORABNIK: Kako se imenujejo očetje domovine Dominikanske republike? ASISTENT: V Dominikanski republiki se za očete države štejejo Juan Pablo Duarte, Francisco del Rosario Sánchez in Matías Ramón Mella. Ti trije možje so vodili boj za neodvisnost Dominikanske republike od Haitija in februarja 1844 ustanovili republiko kot neodvisno državo.\\n\\nJuan Pablo Duarte velja za ustanovitelja Dominikanske republike in je v državi zelo cenjen narodni heroj. Francisco del Rosario Sánchez in Matías Ramón Mella prav tako veljata za pomembna narodna junaka zaradi njune vloge v boju za neodvisnost.\\n\\nTi trije možje so sestavni del zgodovine in kulture Dominikanske republike ter se jih spominjajo in častijo zaradi njihove žrtve in odločnosti v boju za neodvisnost in svobodo svoje države.'''\n","# Tokenize the custom reply\n","inputs = tokenizer.encode_plus(custom_reply, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n","\n","# Perform inference\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Get the predicted label\n","predicted_label = torch.argmax(outputs.logits).item()\n","\n","# Print the predicted label\n","print(\"Predicted label:\", predicted_label)\n","probas = torch.nn.functional.softmax(outputs.logits, dim=1)[0]\n","s  = probas[0]^1 + probas[1]^2 + probas[2]^3 + probas[3]^4 + probas[4]^5\n","print(\"Predictes probability: \" + s)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n","import torch\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from datasets import *"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["split = \"test\"\n","data_path_pairs = '../../data/results/prompt_reply_pairs_5_generated_test_t5-sl-small.csv'\n","num_return_sequences = 5"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["data = pd.read_csv(data_path_pairs, sep=\";\")\n","eval_data = Dataset.from_pandas(data[[\"prompt\", \"generated\"]])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"vh-student/sloberta-si-rrhf\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"vh-student/sloberta-si-rrhf\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def convert_to_features(examples):\n","    prefix_in = \"Uporabnik: \"\n","    examples[\"prompt\"] = [prefix_in + prompt for prompt in examples[\"prompt\"]]\n","    prefix_out = \"Asistent: \"\n","    examples[\"generated\"] = [prefix_out + reply for reply in examples[\"generated\"]]\n","    \n","    examples[\"PROMPT\"] = [prompt + \" \" + reply for prompt, reply in zip(examples[\"prompt\"], examples[\"generated\"])]\n","    model_inputs = tokenizer(examples['PROMPT'], pad_to_max_length=True, max_length=512, truncation=True, return_tensors='pt')\n","\n","    return model_inputs"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['prompt', 'generated', 'PROMPT', 'input_ids', 'attention_mask'],\n","    num_rows: 24375\n","})"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bfd11c2096548d9abd884fcdd1ce031","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/24375 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'PROMPT': 'Uporabnik: Vzemimo naključno matriko $n\\\\krat n$ z realnimi elementi, od katerih je vsak element izbran neodvisno od drugih iz standardne normalne porazdelitve. Kako oceniti verjetnost, da imajo vse lastne vrednosti te matrike pozitiven realni del? Kakšno je asimptotično obnašanje te verjetnosti, ko $n$ teži k neskončnosti? Asistent: ݄',\n"," 'input_ids': tensor([    5, 23231, 31825, 23231, 31825, 27679, 17440, 16551,  9235,    20,\n","         11791, 31778, 31895,   377,  1512, 31890,    19,   131, 12103,  6448,\n","         31791,    87,  1113,    38,  1154,  6724, 10565, 11913,    87,   989,\n","            76, 22063, 21425, 19932, 31795,  2406, 12829,  7891, 31791,    67,\n","           777,   197,  4777,  1417,    71, 27617,  9510, 21932,   691, 31846,\n","         20026,    38,  7347,  9041,  6513, 10944,    71, 11351, 31791,    68,\n","         11791, 31778, 31890, 12367,    30, 10681,  2123, 31846,   180, 10482,\n","          2202, 31825,   180, 10482,  2202, 31825, 31773,     3,     6,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1]),\n"," 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0])}"]},"metadata":{},"output_type":"display_data"}],"source":["eval_data = eval_data.map(convert_to_features, batched=True, load_from_cache_file=False)\n","eval_data.set_format(type=\"torch\", columns=[\"PROMPT\", \"input_ids\", \"attention_mask\"])\n","display(eval_data[0])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/4875 [00:00<?, ?it/s]\n"]},{"ename":"AttributeError","evalue":"'list' object has no attribute 'size'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(indices[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))):\n\u001b[0;32m      8\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> 9\u001b[0m         outputs_proba \u001b[39m=\u001b[39m model(eval_data[indices[i]:indices[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]][\u001b[39m\"\u001b[39;49m\u001b[39mPROMPT\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     10\u001b[0m     probas \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(outputs_proba\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(probas)\n","File \u001b[1;32mc:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\transformers\\models\\camembert\\modeling_camembert.py:1086\u001b[0m, in \u001b[0;36mCamembertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1079\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1086\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1087\u001b[0m     input_ids,\n\u001b[0;32m   1088\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1089\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1090\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1091\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1092\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1093\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1094\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1095\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1096\u001b[0m )\n\u001b[0;32m   1097\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1098\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n","File \u001b[1;32mc:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\rjutr\\miniconda3\\envs\\project_ds_2\\lib\\site-packages\\transformers\\models\\camembert\\modeling_camembert.py:852\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 852\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[0;32m    853\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    854\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n","\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"]}],"source":["# split so it fits on a 12gb gpu - couldn't find a better way (model.generate() options that would do this automatically...)\n","# depends on the model\n","step = 5 # makeshift batch size - idk how to do this better\n","indices = np.concatenate([np.arange(0, eval_data.num_rows, step), [eval_data.num_rows]])\n","\n","outputs = []\n","for i in tqdm(range(len(indices[:-1]))):\n","    with torch.no_grad():\n","        outputs_proba = model(eval_data[indices[i]:indices[i+1]][\"PROMPT\"])\n","    probas = torch.nn.functional.softmax(outputs_proba.logits, dim=1).detach().numpy()[0]\n","    print(probas)\n","    outputs.append(s)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
