{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling, AutoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import datasets as ds\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"../../data/processed/prompt_reply_pairs.csv\", sep=\";\")\n",
    "\n",
    "train_data = data[data[\"split\"] == \"train\"]\n",
    "val_data = data[data[\"split\"] == \"val\"]\n",
    "test_data = data[data[\"split\"] == \"test\"]\n",
    "\n",
    "train_data = ds.Dataset.from_pandas(train_data[['prompt', 'reply']])\n",
    "val_data = ds.Dataset.from_pandas(val_data[['prompt', 'reply']])\n",
    "test_data = ds.Dataset.from_pandas(test_data[['prompt', 'reply']])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cjvt/gpt-sl-base\", truncation=True, truncation_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"cjvt/gpt-sl-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(examples):\n",
    "    prefix_in = \"Uporabnik: \"\n",
    "    # prefix_in = \"\"\n",
    "    examples[\"prompt\"] = [prefix_in + prompt for prompt in examples[\"prompt\"]]\n",
    "    # prefix_out = \"Asistent: \"\n",
    "    prefix_out = \"\"\n",
    "    examples[\"reply\"] = [prefix_out + reply for reply in examples[\"reply\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(examples['prompt'], pad_to_max_length=True, max_length=512, truncation=True, return_tensors='pt')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['reply'], pad_to_max_length=True, max_length=128, truncation=True, return_tensors='pt')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(convert_to_features, batched=True, load_from_cache_file=False)\n",
    "val_data = val_data.map(convert_to_features, batched=True, load_from_cache_file=False)\n",
    "test_data = test_data.map(convert_to_features, batched=True, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = ds.metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/gpt-ft-3\", #The output directory\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    push_to_hub=False, \n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True # load best model at end so we save the best model instead of the last model\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cjvt/gpt-sl-base\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"..\\..\\models\\gpt-ft-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../../data/gpt/test_2.csv\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = test_data[\"prompt\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = tokenizer(sens, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  gen_tokens = model.generate(\n",
    "      **prompts,\n",
    "      do_sample=False,\n",
    "      max_new_tokens=50,\n",
    "  )\n",
    "gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "gen_text = [re.sub('[^a-zA-Z0-9čšž\\ \\.!?,]+', '', _) for _ in gen_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"generated\"] = gen_text\n",
    "test_data.to_csv(\"../../data/gpt_results/test_with_generated_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"prompt\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"generated\"][10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
